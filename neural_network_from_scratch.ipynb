{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a78d72",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "Fully connected deep neural net (DNN) from scratch.<br>\n",
    "**Credits:** [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "We will create two helper functions: `sigmoid` and `sigmoid_dt`. Sigmoid (logistic) function act as the activation function, whereas sigmoid_dt, the derivative, is used on the gradient calculation during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5108e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531fc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_dt(z):\n",
    "    return sigmoid(z)*(1 - sigmoix(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0fe24d",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Here we create the Network class. The constructor takes sizes as a paramenter, which refers to the input nodes at each layer. The initial weights and bias are chosen randomly for each layer initially, with `np.random.randn()` returning a random sample for the normal distribution.\n",
    "\n",
    "`feedforward` function takes a activation vector 'a', loops through all biases and weights, and calculates the activations values for each layer. At the end of the calculation process, it returns the activations for the output layer, the predictions.\n",
    "\n",
    "`SGD`, the stochastic gradient descent funtion, helps traveling down the slope a step at time until the lowest point of the surface, the local minimum of the cost function. The sâ€ ochastic gradient descent uses a mini-batch of the data-points to update the model. The function takes 4 parameters: training data, the epochs, the size of the mini-batches, and the learning rate. Each epoch shuffles the data and creates a mini-batch, that later will be updated with `update_mini_batch`. \n",
    "\n",
    "`backprop` is the backpropagation function. It updates weights and bias at the end an epoch and calculates the error from the previous layer. `backprop` takes a helper function, `cost_derivative`, that determines the error in the output layer.\n",
    "\n",
    "`evaluate` takes the test data as an input and compares predictions with the expected truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b96dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    # sizes = nodes in each layer\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "   \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        training_data = list(training_data)\n",
    "        samples = len(training_data)\n",
    "       \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "       \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size]\n",
    "                            for k in range(0, samples, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "   \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return(output_activations - y)\n",
    "   \n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # stores activations layer by layer\n",
    "        zs = [] # stores z vectors layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "       \n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_dt(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "       \n",
    "        for _layer in range(2, self.num_layers):\n",
    "            z = zs[-_layer]\n",
    "            sp = sigmoid_dt(z)\n",
    "            delta = np.dot(self.weights[-_layer+1].transpose(), delta) * sp\n",
    "            nabla_b[-_layer] = delta\n",
    "            nabla_w[-_layer] = np.dot(delta, activations[-_layer-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "   \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "       \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
